'use strict';
const RAG =
`
import kfp
from kfp import dsl
from kfp.components import func_to_container_op
from typing import NamedTuple

def RAG()-> NamedTuple('Outputs', [("output",str)]):
    import torch
    import requests
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Pinecone
    from langchain_community.document_loaders import PyPDFLoader, OnlinePDFLoader
    import os
    import sys
    from pinecone import Pinecone
    from langchain_pinecone import PineconeVectorStore
    from langchain.embeddings import HuggingFaceEmbeddings
    
    import pinecone
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers import pipeline
    from langchain import HuggingFacePipeline, PromptTemplate
    from langchain.chains import RetrievalQA
    from huggingface_hub import login

    
    url = '%s'#Ex: "https://arxiv.org/pdf/2207.02696.pdf", a yolov7 pdf
    data_name = '%s'#Ex: "/yolov7.pdf"

    huggingface_token = '%s'
    embedding_model = '%s'#Ex: "sentence-transformers/all-MiniLM-L6-v2"
    LLM_model = '%s'#Ex: "openai-community/gpt2" or "meta-llama/Llama-2-7b-chat-hf"
    Pinecone_token = '%s'
    Pinecone_index = '%s'
    
    
    r = requests.get(url, stream=True)
    with open(data_name, 'wb') as fd:
        fd.write(r.content)
    loader = PyPDFLoader(data_name)
    data = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)
    docs = text_splitter.split_documents(data)
    
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
    PINE_KEY = os.environ.get("PINECONE_API_KEY", Pinecone_token)
    pc = Pinecone(api_key=Pinecone_token)
    os.environ["PINECONE_API_KEY"] = Pinecone_token
    index = pc.Index(Pinecone_index)
    index_name = Pinecone_index
    docsearch = PineconeVectorStore.from_documents(docs, embedding=embeddings, index_name=index_name)

    
    login(huggingface_token)
    
    
    tokenizer = AutoTokenizer.from_pretrained(LLM_model)
    model = AutoModelForCausalLM.from_pretrained(LLM_model,
                        device_map='auto',
                        torch_dtype=torch.float16,
                        use_auth_token=True,
                        load_in_8bit=True # If you don't use GPU, comment this parameter
                         )
    model = pipeline("text-generation",
                model=model,
                tokenizer= tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens = 512,
                do_sample=True,
                top_k=30,
                num_return_sequences=3,
                eos_token_id=tokenizer.eos_token_id
                )
    
    llm=HuggingFacePipeline(pipeline=model, model_kwargs={'temperature':0.1})
    
    SYSTEM_PROMPT = """Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer."""
    B_INST, E_INST = "[INST]", "[/INST]"
    B_SYS, E_SYS = "<<SYS>>\\n", "\\n<</SYS>>\\n\\n"
    
    SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT + E_SYS
    instruction = """
    {context}

    Question: {question}
    """
    template = B_INST + SYSTEM_PROMPT + instruction + E_INST
    
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])
    
    qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=docsearch.as_retriever(search_kwargs={"k":3}),
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},)
    
    question = questions
    result = qa_chain(question)
    
    print(result['result'])

    output = result['result']
    return(output)

RAG_op = func_to_container_op(RAG,packages_to_install = [
    'requests',
    'langchain',
    'langchain-community',
    'pypdf',
    'pinecone-client',
    'langchain_pinecone',
    'sentence_transformers',
    'googletrans==3.1.0a0',
    'transformers',
    'torch',
    'bitsandbytes',
    'accelerate',
    'urllib3==1.26.15'
])

@dsl.pipeline()
def rag_pipeline():
    rag_task = RAG_op().set_gpu_limit(1)

from kfp import compiler

compiler.Compiler().compile(rag_pipeline, 'RAG_pipeline.yaml')

import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = "%s"
username = "%s"
password = "%s"

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True

namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for RAG."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile="RAG_pipeline.yaml"
    name='pipeline-' + random_suffix()
    description="This is a RAG pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'RAG' + random_suffix()
    description = "This is a RAG run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)

    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    


    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)
            #print(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions'] # Comfirm completion.
                    
            except KeyError:
                nodes = {}
                conditions = []

            output_value = None
            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False
            
            '''''
            def find_all_keys(node):
                if isinstance(node, dict):
                    for key in node.keys():
                        print("Key:", key)
                        find_all_keys(node[key])
                elif isinstance(node, list):
                    for item in node:
                        find_all_keys(item)

            # Call the function with your JSON data
            find_all_keys(output)
            '''''

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Pipeline is still running. Waiting...")
            time.sleep(polling_interval-1)
    
    found_final_pvc_name = False  # Add a variable to track if the PVC name has been found

    def find_final_pvc_name(node):
        global found_final_pvc_name  # Declare the variable as global

        if not found_final_pvc_name:  # If the PVC name has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'mypvc-name':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_final_pvc_name = True  # Set to True after finding the PVC name
                                print("mypvc-name:", value)
                                return value
                for key, value in node.items():
                    result = find_final_pvc_name(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_final_pvc_name(item)
                    if result:
                        return result

        return None
    
    find_final_pvc_name(output)  # Call the function to find final_pvc_name


    found_model_func_accuracy = False

    def find_model_func_accuracy(node):
        global found_model_func_accuracy  # Declare the variable as global

        if not found_model_func_accuracy:  # If the model-func-accuracy has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'model-func-accuracy':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_model_func_accuracy = True  # Set to True after finding model-func-accuracy
                                print("Model Accuracy:", value)
                                return value

                for key, value in node.items():
                    result = find_model_func_accuracy(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_model_func_accuracy(item)
                    if result:
                        return result

        return None
    
    find_model_func_accuracy(output)
`;




module.exports = {
    RAG
};
